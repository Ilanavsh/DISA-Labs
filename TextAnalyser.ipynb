{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextAnalyser.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilanavsh/DISA-Labs/blob/master/TextAnalyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZHW_7_-UKFb",
        "colab_type": "code",
        "outputId": "057f29fd-7c6f-4b2d-fb41-0d2799e90b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "!pip install regex\n",
        "import regex as re\n",
        "!python -m spacy download en_core_web_sm\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.6.8\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bEE4OUYUL1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextAnalyser():\n",
        "    '''\n",
        "    Text Analyser defines and performs coomon text analysis tasks\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # load spaCy and the English model\n",
        "        self._nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        pass\n",
        "\n",
        "    def parseText(self):\n",
        "        ''' parse the text read previously by read functions '''\n",
        "        # parse text\n",
        "        self._doc = self._nlp(self._text)\n",
        "        pass\n",
        "    \n",
        "    def readLinesAndParse(self, fileName):\n",
        "        f=open(fileName, \"r\")\n",
        "        self._lineList = f.readlines()\n",
        "        self._docList = []\n",
        "        for line in self._lineList:\n",
        "            self._docList.append(self._nlp(line))\n",
        "        pass\n",
        "\n",
        "    def printDocList(self, maxNumer=100):\n",
        "        '''\n",
        "        print each doc in self._docList (max 100 docs)\n",
        "        readLinesAndParse must be invoke first to populate the docList\n",
        "        '''\n",
        "        i = 0\n",
        "        for doc in self._docList:\n",
        "            print(doc)\n",
        "            for token in doc:\n",
        "                print(token)\n",
        "            i += 1\n",
        "            if i > maxNumer:\n",
        "                break\n",
        "        pass\n",
        "\n",
        "    def readAllText(self, fileName):\n",
        "        ''' read the file named in the paramater fileName\n",
        "        '''\n",
        "        f=open(fileName, \"r\")\n",
        "        self._text = f.read()\n",
        "        pass\n",
        "\n",
        "    def printDocSimilarity(self, maxNumber=100):\n",
        "        i = 0\n",
        "        for doc1 in self._docList:\n",
        "            for doc2 in self._docList:\n",
        "                print(doc1)\n",
        "                print(doc2)\n",
        "                print(doc1.similarity(doc2))\n",
        "            i += 1\n",
        "            if i > maxNumber:\n",
        "                break\n",
        "        pass\n",
        "\n",
        "    # remove quatation marks\n",
        "    def removeQutationMarks(self):\n",
        "        self._text = re.sub(r'[=\"]', '', self._text)\n",
        "        pass\n",
        "\n",
        "    def printText(self):\n",
        "        print(self._text)\n",
        "\n",
        "    def printTokens(self, number=100):\n",
        "        i = 0\n",
        "        for token in self._doc:\n",
        "            print(token.text, token.has_vector, token.vector_norm)\n",
        "            i += 1\n",
        "            if(i >= number):\n",
        "                break\n",
        "        pass\n",
        "    \n",
        "    def printTokenVector(self, number=1):\n",
        "        i = 0\n",
        "        for token in self._doc:\n",
        "            print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "            print(token.vector.shape)\n",
        "            print(token.vector)\n",
        "            i += 1\n",
        "            if(i > number):\n",
        "                break\n",
        "        pass\n",
        "\n",
        "    def printTokenSimilarities(self, number=100):\n",
        "        i = 0\n",
        "        for token1 in self._doc:\n",
        "            for token2 in self._doc:\n",
        "                print(token1.text, token2.text, token1.similarity(token2))\n",
        "            i += 1\n",
        "            if(i > number):\n",
        "                break\n",
        "        pass\n",
        "\n",
        "    def displayNER(self):\n",
        "        displacy.render(self._doc, style = \"ent\",jupyter=True)\n",
        "    \n",
        "    def printNE01(self):\n",
        "        for i, s in enumerate(self._doc.sents):\n",
        "            print('%2d: %s' % (i, re.sub(r'\\n+', '', s.text)))\n",
        "            if s.as_doc().ents:\n",
        "                print('-'*80)\n",
        "                for e in s.as_doc().ents:\n",
        "                    print('%-11s: %s' % (e.label_, re.sub(r'\\n+', '', e.text)))\n",
        "            print('='*80)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SjOtOkoUUVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# analyse products\n",
        "def analyseProductCatgeoris():\n",
        "    textAnalyser = TextAnalyser()\n",
        "    textAnalyser.readLinesAndParse('./Data/products.csv')\n",
        "    textAnalyser.printDocSimilarity(10)\n",
        "\n",
        "# analyse simple sample text for expermintation\n",
        "def analyseSimpleSample01():\n",
        "    textAnalyser = TextAnalyser()\n",
        "    textAnalyser.readAllText('./Data/simpleSample01.txt')\n",
        "    textAnalyser.parseText()\n",
        "    # textAnalyser.printText()\n",
        "    # textAnalyser.printNE01()\n",
        "#     textAnalyser.printTokens()\n",
        "    textAnalyser.printTokenSimilarities()\n",
        "#     textAnalyser.printTokenVector()\n",
        "\n",
        "# analyse simple sample text for expermintation\n",
        "def analyseSimpleSample02():\n",
        "    textAnalyser = TextAnalyser()\n",
        "    textAnalyser.readAllText('./Data/simpleSample02.txt')\n",
        "    textAnalyser.parseText()\n",
        "    # textAnalyser.printText()\n",
        "    # textAnalyser.printNE01()\n",
        "    # textAnalyser.printTokens()\n",
        "    textAnalyser.printTokenSimilarities()\n",
        "    pass\n",
        "\n",
        "# analyse NCC text for expermintation\n",
        "def analyseNccText():\n",
        "    textAnalyser = TextAnalyser()\n",
        "    textAnalyser.readAllText('./Data/ncc-1701-D.txt')\n",
        "    textAnalyser.parseText()\n",
        "    # textAnalyser.printText()\n",
        "    # textAnalyser.printNE01()\n",
        "    # textAnalyser.printTokens()\n",
        "    # textAnalyser.printTokenSimilarities()\n",
        "    textAnalyser.displayNER()\n",
        "\n",
        "analyseProductCatgeoris()\n",
        "# analyseSimpleSample01()\n",
        "# analyseSimpleSample02()\n",
        "# analyseNccText()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}